[
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "Let us first define some functions used throughout the notebook. We don’t include the imports for the sake of space, but please look at the notebook nbs/analysis.ipynb for details.\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=0, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))\n\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()\n\nLet us also define some variables:\n\npath = Path('data/datasets/autos.csv')\nresults_path = Path('results/used_cars')\nresults_path.mkdir (parents=True, exist_ok=True)\n\ntarget_variable = 'sale_duration'\noutliers = dict (numeric_variables_high=0.99,\n                 numeric_variables_low=0.01,\n                 date_created=True)\nadd_datepart_flag = True\n\n\ncd ..\n\n/home/jaumeamllo/workspace/mine/tabularml"
  },
  {
    "objectID": "analysis.html#load-data",
    "href": "analysis.html#load-data",
    "title": "Analysis",
    "section": "Load data",
    "text": "Load data\nThe first step is to load the data and have a quick look at it:\n\ndf = pd.read_csv(path, encoding=\"ISO-8859-1\", parse_dates=['dateCrawled', 'dateCreated', 'lastSeen'])\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      dateCrawled\n      name\n      seller\n      offerType\n      price\n      abtest\n      vehicleType\n      yearOfRegistration\n      gearbox\n      powerPS\n      model\n      kilometer\n      monthOfRegistration\n      fuelType\n      brand\n      notRepairedDamage\n      dateCreated\n      nrOfPictures\n      postalCode\n      lastSeen\n    \n  \n  \n    \n      0\n      2016-03-24 11:52:17\n      Golf_3_1.6\n      privat\n      Angebot\n      480\n      test\n      NaN\n      1993\n      manuell\n      0\n      golf\n      150000\n      0\n      benzin\n      volkswagen\n      NaN\n      2016-03-24\n      0\n      70435\n      2016-04-07 03:16:57\n    \n    \n      1\n      2016-03-24 10:58:45\n      A5_Sportback_2.7_Tdi\n      privat\n      Angebot\n      18300\n      test\n      coupe\n      2011\n      manuell\n      190\n      NaN\n      125000\n      5\n      diesel\n      audi\n      ja\n      2016-03-24\n      0\n      66954\n      2016-04-07 01:46:50\n    \n    \n      2\n      2016-03-14 12:52:21\n      Jeep_Grand_Cherokee_\"Overland\"\n      privat\n      Angebot\n      9800\n      test\n      suv\n      2004\n      automatik\n      163\n      grand\n      125000\n      8\n      diesel\n      jeep\n      NaN\n      2016-03-14\n      0\n      90480\n      2016-04-05 12:47:46\n    \n    \n      3\n      2016-03-17 16:54:04\n      GOLF_4_1_4__3TÜRER\n      privat\n      Angebot\n      1500\n      test\n      kleinwagen\n      2001\n      manuell\n      75\n      golf\n      150000\n      6\n      benzin\n      volkswagen\n      nein\n      2016-03-17\n      0\n      91074\n      2016-03-17 17:40:17\n    \n    \n      4\n      2016-03-31 17:25:20\n      Skoda_Fabia_1.4_TDI_PD_Classic\n      privat\n      Angebot\n      3600\n      test\n      kleinwagen\n      2008\n      manuell\n      69\n      fabia\n      90000\n      7\n      diesel\n      skoda\n      nein\n      2016-03-31\n      0\n      60437\n      2016-04-06 10:17:21"
  },
  {
    "objectID": "analysis.html#cleaning-and-eda",
    "href": "analysis.html#cleaning-and-eda",
    "title": "Analysis",
    "section": "Cleaning and EDA",
    "text": "Cleaning and EDA\nWe first gather the numerical and categorical columns:\n\nnumerical_variables = [\n    x for x in df.dtypes.index \n    if (str(df.dtypes[x]).startswith ('int') or str(df.dtypes[x]).startswith ('float')) and x != 'postalCode'\n]\nnumerical_variables\n\n['price',\n 'yearOfRegistration',\n 'powerPS',\n 'kilometer',\n 'monthOfRegistration',\n 'nrOfPictures']\n\n\n\ncategorical_variables = set (df.columns).difference (numerical_variables)\ncategorical_variables\n\n{'abtest',\n 'brand',\n 'dateCrawled',\n 'dateCreated',\n 'fuelType',\n 'gearbox',\n 'lastSeen',\n 'model',\n 'name',\n 'notRepairedDamage',\n 'offerType',\n 'postalCode',\n 'seller',\n 'vehicleType'}\n\n\n\ncategorical_variables = categorical_variables.difference ({'dateCrawled', 'dateCreated', 'name', 'lastSeen'})\ncategorical_variables = list (categorical_variables)\n\n\nOutliers\nNext, we remove outliers found in the data. For that purpose, we first examine the statistics of numerical variables:\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      price\n      yearOfRegistration\n      powerPS\n      kilometer\n      monthOfRegistration\n      nrOfPictures\n      postalCode\n    \n  \n  \n    \n      count\n      3.715280e+05\n      371528.000000\n      371528.000000\n      371528.000000\n      371528.000000\n      371528.0\n      371528.00000\n    \n    \n      mean\n      1.729514e+04\n      2004.577997\n      115.549477\n      125618.688228\n      5.734445\n      0.0\n      50820.66764\n    \n    \n      std\n      3.587954e+06\n      92.866598\n      192.139578\n      40112.337051\n      3.712412\n      0.0\n      25799.08247\n    \n    \n      min\n      0.000000e+00\n      1000.000000\n      0.000000\n      5000.000000\n      0.000000\n      0.0\n      1067.00000\n    \n    \n      25%\n      1.150000e+03\n      1999.000000\n      70.000000\n      125000.000000\n      3.000000\n      0.0\n      30459.00000\n    \n    \n      50%\n      2.950000e+03\n      2003.000000\n      105.000000\n      150000.000000\n      6.000000\n      0.0\n      49610.00000\n    \n    \n      75%\n      7.200000e+03\n      2008.000000\n      150.000000\n      150000.000000\n      9.000000\n      0.0\n      71546.00000\n    \n    \n      max\n      2.147484e+09\n      9999.000000\n      20000.000000\n      150000.000000\n      12.000000\n      0.0\n      99998.00000\n    \n  \n\n\n\n\n\nWe see outliers in:\n\n\nPrice: minimum value 0, and maximum value > 2000M euros\nYear of registration: minimum value 1000, and maximum value > 3000\nMonth of registration: minimum value 0\npowerPS: minimum value 0, maximum value 20K\n\n\nWe also see that nrOfPictures is always zero, so that it is not relevant\n\nWe have two possibilities:\n\nRemove rows with outliers\nTreat outlier values as missing values\n\nIn this analysis, we go for the second option.\nTypical outlier detection methods:\n\nBased on percentile\nBased on z-score\n\nIn this analysis, we go for the first option:\n\ndf_clean = df.copy()\ndf_clean[numerical_variables] = df_clean[numerical_variables].apply (\n    lambda x: x.mask ((x < x.quantile(outliers['numeric_variables_low'])) | (x > x.quantile(outliers['numeric_variables_high'])))\n)\n\n# see if there are rows that have all values missing and need to be removed, and\n# check maximum ratio of missing values in one row\ndf_clean[numerical_variables].isna().all(axis=1).any(), df_clean[numerical_variables].isna().mean(axis=1).max()\n\n(False, 0.5)\n\n\n\n\nRemove nrOfPictures\nWe remove the column nrOfPictures, since it is not relevant:\n\ndf_clean = df_clean.drop (columns='nrOfPictures')\nnumerical_variables.remove ('nrOfPictures')\n\n\n\ndateCreated\nWe examine the column dateCreated:\n\ndf_clean.dateCreated.max()-df_clean.dateCreated.min()\n\nTimedelta('759 days 00:00:00')\n\n\nThe time span is more than 2 years. However, we see below that there are only few cases where the ad was posted more than few months ago, while the rest are just a few months old. To see that, we transform dateCreated to the offset in terms of number of months since the earliest ad in the dataset:\n\ndf_clean['offset_date'] = (df_clean['dateCreated']-df_clean['dateCreated'].min()).dt.days\n\n\ndf_clean.offset_date.hist()\n\n<Axes: >\n\n\n\n\n\nClearly, almost all the ads are in one bin. Let’s look at this:\n\nnumber_ads, bins = np.histogram (df_clean.offset_date)\npd.DataFrame ([number_ads, bins[1:]], index=['number_ads','bins'])\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n    \n  \n  \n    \n      number_ads\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      4.0\n      5.0\n      53.0\n      371464.0\n    \n    \n      bins\n      75.9\n      151.8\n      227.7\n      303.6\n      379.5\n      455.4\n      531.3\n      607.2\n      683.1\n      759.0\n    \n  \n\n\n\n\nIn the table above, we see that the last bin is where almost all the ads are, followed by the second to last bin, with few ads. From the 5th bin downwards, there are only 2 ads.\n\nvery_old_ads = df_clean[df_clean.offset_date < 450]\nother_ads = df_clean[df_clean.offset_date >= 450]\n\nprint ('oldest ads created between: ', very_old_ads.dateCreated.min(),very_old_ads.dateCreated.max())\n\nprint ('remaining ads created between: ', other_ads.dateCreated.min(),other_ads.dateCreated.max())\n\noldest ads created between:  2014-03-10 00:00:00 2015-03-20 00:00:00\nremaining ads created between:  2015-06-11 00:00:00 2016-04-07 00:00:00\n\n\nWe have four options:\n- no outlier\n- oldest (2014) is outlier\n- two oldest (before mid year 2016) are outliers.\n- all but the ones in the last two bins (with 53 and 371464 rows) are outliers \n- use a quantile as done for the other numerical variables.\nIn this analysis, we use the third option: treat the two oldest ads as two outliers:\n\nif outliers['date_created']:\n    df_clean = df_clean[df_clean.dateCreated >= '2015-06-01']\n\n\ndf_clean = df_clean.drop (columns = 'offset_date')\n\n\n\nyearRegistration + monthRegistration\nWe use yearOfRegistration and monthOfRegistration to calculate the age of the vehicle, in number of months since the ad was posted. We first look if the outlier removal removed suspicious low and high values:\n\ndf_clean[['monthOfRegistration','yearOfRegistration']].describe().loc[['min','max']]\n\n\n\n\n\n  \n    \n      \n      monthOfRegistration\n      yearOfRegistration\n    \n  \n  \n    \n      min\n      0.0\n      1978.0\n    \n    \n      max\n      12.0\n      2018.0\n    \n  \n\n\n\n\nThere are still months with value 0, we treat them as missing values. We impute them using the middle of the year, month 6.\n\ndf_clean.loc [df_clean.monthOfRegistration==0, 'monthOfRegistration'] = 6\n\n\ndf_clean['age'] = pd.to_datetime(df_clean.dateCreated.max()) - pd.to_datetime ({\n    'month': df_clean.monthOfRegistration,\n    'year': df_clean.yearOfRegistration,\n    'day': np.tile (15, df_clean.shape[0])}\n)\n\ndf_clean['age'] = df_clean['age'].dt.days\n\n\nnumerical_variables.remove ('yearOfRegistration')\nnumerical_variables.remove ('monthOfRegistration')\nnumerical_variables.append ('age')\n\n\n\nTarget variable\n\ndf_clean[target_variable] = (df_clean ['lastSeen'] - df_clean['dateCreated']).dt.days\n\n\n\ndate information\nDepending on whether the ad was posted on a weekend, on holidays, etc., more people might be able to look at it and it might be sold more quickly. We add this information here:\n\nif add_datepart_flag:\n    country_holidays = holidays.country_holidays('DE')\n    df_clean['holidays'] = [int(day in country_holidays) for day in df_clean.dateCreated]\n    df_clean['day_of_week'] = df_clean.dateCreated.dt.dayofweek.values\n\n\nnumerical_variables.append ('day_of_week')\ncategorical_variables.append ('holidays')\n\n\njoblib.dump ([df_clean, numerical_variables, categorical_variables], results_path / 'df_clean.pkl')\n\n['results/used_cars/df_clean.pkl']\n\n\n\n\nHistograms\nWe look at the resulting histograms to see if there are clear outliers. This doesn’t seem to be the case.\n\ndf_clean['price'].hist()\n\n<Axes: >\n\n\n\n\n\n\ndf_clean['age'].hist()\n\n<Axes: >\n\n\n\n\n\n\ndf_clean['powerPS'].hist()\n\n<Axes: >\n\n\n\n\n\n\ndf_clean['kilometer'].hist()\n\n<Axes: >"
  },
  {
    "objectID": "analysis.html#split-data-and-preprocess",
    "href": "analysis.html#split-data-and-preprocess",
    "title": "Analysis",
    "section": "Split data and preprocess",
    "text": "Split data and preprocess\nThe first step is to split the data in training and validation. For simplicity, e don’t use a test set here, although we should use it if we wanted to estimate the final accuracy after selecting our best model on the validation set.\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_idx, valid_idx = train_test_split(np.arange(df_clean.shape[0]), test_size=0.30, random_state=42)\njoblib.dump ([train_idx, valid_idx], results_path / 'indexes.pkl')\n\n['results/used_cars/indexes.pkl']\n\n\nThe second step is to preprocess the data. We do that with the help of TabularPandas in the fastai library:\n\nUsing Categorify, we replace columns in categorical_variables list with numeric categorical columns. We use just an discrete value instead of using a one-hot encoding. This tends to work better for decision trees and random forests, as explored in “Splitting on Categorical Predictors in Random Forests”\nUsing FillMissing, we replace missing values with the median, and we create a boolean column that is True for any row where the value was missing\n\n\nprocs = [Categorify, FillMissing]\ntabular = TabularPandas (df_clean, procs, categorical_variables, numerical_variables, y_names=target_variable, splits=(list(train_idx),list(valid_idx)))\n\n\npath = Path('data/datasets')\njoblib.dump (tabular, path / 'tabular.pkl')\n\n['data/datasets/tabular.pkl']"
  },
  {
    "objectID": "analysis.html#decision-tree",
    "href": "analysis.html#decision-tree",
    "title": "Analysis",
    "section": "Decision Tree",
    "text": "Decision Tree\nWe start by fitting a very simple yet powerful type of model, the decision tree. A good characteristic of this type of model is that it is interpretable and allows to analyze the data and the important variables.\n\ntabular = joblib.load (path / 'tabular.pkl')\n\n\nX, y = tabular.train.xs, tabular.train.y\nvalid_X, valid_y = tabular.valid.xs, tabular.valid.y\n\n\nVisualization\nWe can visualize on which basis the tree splits the data. We see that:\n- The most important predictor is the `price`: the lower the price, the lower the duration of the sale. \n- The data where most errors happen is the one for large durations. This might be due to the few cases with a long duration, which could be considered almost outliers.\n\nm = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit (X, y);\n\n\ndraw_tree(m, X, size=10, leaves_parallel=True, precision=2)\n\nWarning: Could not load \"/home/jaumeamllo/miniconda3/envs/tsforecast/bin/../lib/graphviz/libgvplugin_pango.so.6\" - It was found, so perhaps one of its dependents was not.  Try ldd.\nWarning: no value for width of non-ASCII character 226. Falling back to width of space character\n\n\n\n\n\nWe can visualize the same using the dtreeviz library:\n\nsamp_idx = np.random.permutation(len(y))[:500]\nviz = dtreeviz.model (m, X.iloc[samp_idx], y.iloc[samp_idx], feature_names=X.columns, target_name=target_variable)\n\n\nviz.view (fontname='DejaVu Sans', scale=2, label_fontsize=16,\n        orientation='LR')\n\n/home/jaumeamllo/miniconda3/envs/tsforecast/lib/python3.10/site-packages/sklearn/base.py:420: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\nWarning: Could not load \"/home/jaumeamllo/miniconda3/envs/tsforecast/bin/../lib/graphviz/libgvplugin_pango.so.6\" - It was found, so perhaps one of its dependents was not.  Try ldd.\nWarning: no value for width of non-ASCII character 226. Falling back to width of space character\n\n\n\n\n\n\n\nTraining\nLet’s first start by training a full size decision tree.\n\ndt_model = DecisionTreeRegressor()\ndt_model.fit(X, y);\n\n\n\nEvaluation\nWe use the root of the mean squared error as evaluation metric.\n\ndef r_mse (pred, y): \n    return round(math.sqrt(((pred-y)**2).mean()), 6)\ndef m_rmse (dt_model, X, y): \n    return r_mse(dt_model.predict(X), y)\n\nLet’s see at the error on the training set:\n\nm_rmse (dt_model, X, y)\n\n0.283855\n\n\n… and on the validation set:\n\nm_rmse (dt_model, valid_X, valid_y)\n\n11.614361\n\n\nWe can see that we are clearly overfitting. Let’s look at the number of leaves, and compare it against the total number of observations:\n\ndt_model.get_n_leaves(), len(X)\n\n(201472, 260068)\n\n\nThe number of leaves is similar to the number of observations. We need to reduce the size of the decision tree:\n\ndt_model = DecisionTreeRegressor (min_samples_leaf=25)\ndt_model.fit (tabular.train.xs, tabular.train.y)\nm_rmse (dt_model, X, y), m_rmse(dt_model, valid_X, valid_y)\n\n(7.683234, 8.817406)\n\n\nWe get a much better error on validation set, let’s see the number of leaves:\n\ndt_model.get_n_leaves()\n\n7949"
  },
  {
    "objectID": "analysis.html#random-forest",
    "href": "analysis.html#random-forest",
    "title": "Analysis",
    "section": "Random Forest",
    "text": "Random Forest\nWe have built a baseline using Decision Trees. Let us explore now the use of Random Rorests\n\nTraining\n\ndef fitted_rf (X, y, n_estimators=128, max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor (\n        n_jobs=-1, n_estimators=n_estimators, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True, **kwargs\n    ).fit(X, y)\n\n\nrf_model = fitted_rf (X, y);\njoblib.dump (rf_model, results_path / 'rf_model1.pkl')\n\n\n\nEvaluation\n\nm_rmse (rf_model, X, y), m_rmse(rf_model, valid_X, valid_y), r_mse (rf_model.oob_prediction_, y)\n\n(6.048382, 8.272029, 8.215691)\n\n\nWe see whether using another rule like the sqrt for calculating max_features per tree improves the model:\n\nrf_model_2 = fitted_rf (X, y, max_features='sqrt');\nm_rmse (rf_model_2, X, y), m_rmse(rf_model_2, valid_X, valid_y), r_mse(rf_model_2.oob_prediction_, y)\n\n(6.649456, 8.296415, 8.233054)\n\n\nIt doesnt’ improve. We could play with the ratio of max features, min samples per leaf, and other hyper-parameters. We leave this as future work.\n\n\nVisualization\nLet’s see the impact of n_estimators on the performance:\n\nimport warnings\nwarnings.filterwarnings ('ignore')\npreds = np.stack([t.predict(valid_X) for t in rf_model.estimators_])\nplt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(128)]);\n\n\n\n\nWe see that the accuracy plateaus from 80 estimators onwards\n\n\nInterpretation\n\nConfidence\nWe look at the standard deviation of the predictions across the different trees. For those sales where there is low standard deviation, most of the trees agree on the sale duration estimate. This is useful in production, to maybe avoid providing a predition for those requests where there is little agreement on the sale duration.\n\npreds = np.stack([t.predict(valid_X) for t in rf_model.estimators_])\npreds_std = preds.std(0)\n\n\nplt.hist (preds_std);\n\n\n\n\nWe see that there is in general high standard deviation for most sales, except for few cases.\n\n\nFeature importance\nLet’s look at what features are most important for the regression task:\n\ndef rf_feat_importance (rf_model, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':rf_model.feature_importances_}).sort_values('imp', ascending=False)\n\n\nimportance = rf_feat_importance (rf_model, X)\nimportance\n\n\n\n\n\n  \n    \n      \n      cols\n      imp\n    \n  \n  \n    \n      7\n      postalCode\n      0.193067\n    \n    \n      14\n      price\n      0.184392\n    \n    \n      17\n      age\n      0.153438\n    \n    \n      15\n      powerPS\n      0.099052\n    \n    \n      6\n      model\n      0.088209\n    \n    \n      0\n      brand\n      0.061477\n    \n    \n      18\n      day_of_week\n      0.060955\n    \n    \n      3\n      vehicleType\n      0.042012\n    \n    \n      16\n      kilometer\n      0.036495\n    \n    \n      9\n      abtest\n      0.023295\n    \n    \n      4\n      fuelType\n      0.016574\n    \n    \n      8\n      gearbox\n      0.015988\n    \n    \n      5\n      notRepairedDamage\n      0.015318\n    \n    \n      10\n      holidays\n      0.006917\n    \n    \n      11\n      price_na\n      0.001413\n    \n    \n      13\n      age_na\n      0.000947\n    \n    \n      12\n      powerPS_na\n      0.000451\n    \n    \n      1\n      seller\n      0.000000\n    \n    \n      2\n      offerType\n      0.000000\n    \n  \n\n\n\n\nWe see that the most important features are:\n- postalCode: probably due to the higher density of population, and thus buyers, in certain areas.\n- price: cheaper cars are sold faster.\n- age: newer is better\nSurprisingly, the day of the week plays a role even more important than the number of kilometers, to be sold faster, maybe because people look at ads more at certain days of the week.\nLet’s look at the same graphically:\n\ndef plot_feature_importance (importance):\n    return importance.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_feature_importance(importance);\n\n\n\n\n\n\n\nRemoving variables\nWe see if we can improve the performance by removing those features that have very little importance and might be adding noise:\n\ndef evaluate_removing_variables (importance, X, y, valid_X, valid_y, threhsold):\n    to_keep = importance[importance.imp>threhsold].cols\n    X_imp = X[to_keep]\n    valid_X_imp = valid_X[to_keep]\n\n    rf_model = fitted_rf(X_imp, y)\n    print (m_rmse(rf_model, X_imp, y), m_rmse(rf_model, valid_X_imp, valid_y), r_mse (rf_model.oob_prediction_, y))\n    return rf_model, X_imp, valid_X_imp, to_keep\n\n\nrf_model, X_imp, valid_X_imp, to_keep = evaluate_removing_variables (importance, X, y, valid_X, valid_y, 0.0001)\njoblib.dump (rf_model, results_path / 'removing_low_importance.pkl')\n\n6.045863 8.268369 8.211243\n\n\n['results/used_cars/removing_low_importance.pkl']\n\n\nThere is almost no improvement, let’s see a bit more agressive pruning:\n\nrf_model2, X_imp2, valid_X_imp2, to_keep2 = evaluate_removing_variables (importance, X, y, valid_X, valid_y, 0.001)\n\n6.050095 8.272465 8.213242\n\n\n\nrf_model2, X_imp2, valid_X_imp2, to_keep2 = evaluate_removing_variables (importance, X, y, valid_X, valid_y, 0.005)\n\n6.010566 8.271691 8.210651\n\n\nThe best threshold seems to be 0.0001. Let’s use it and look at the feature importance again, after removing variables with low importance:\n\nimportance = rf_feat_importance(rf_model, X_imp)\nplot_feature_importance(importance);\n\n\n\n\n\n\nRemoving redundant variables\nWe see if there are variables that might be closely correlated, in terms of ranking:\n\ncluster_columns(X_imp)\n\n\n\n\nPowerPS and price seem to be quite correlated. We see what happens when we remove each of the variables in turn. To do it quickly, we use the out-of-bag score, by using a random forest where each tree is trained on a smaller subset of data, so that the out-of-bag score is measured on the remaining subset. This score indicates the generalization provided, being the higher the better.\n\ndef get_oob (df, y):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df, y)\n    return m.oob_score_\n\nLet’s see the score of the original features:\n\ndef fitted_rf2 (X, y):\n    return RandomForestRegressor (\n        n_estimators=40, min_samples_leaf=15,\n        max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True).fit(X, y)\n\n\noriginal_oob = get_oob(X_imp, y)\noriginal_oob\n\n0.06008218588804892\n\n\n\noob_scores = {c:get_oob(X_imp.drop(c, axis=1), y) for c in (importance.cols)}\noob_scores\n\n{'postalCode': 0.058280710022504234,\n 'price': 0.0415493436634512,\n 'age': 0.05244495695648832,\n 'powerPS': 0.059019327633660135,\n 'model': 0.0593482908544406,\n 'brand': 0.058739696982127,\n 'day_of_week': 0.05945691943436404,\n 'vehicleType': 0.05943518125387026,\n 'kilometer': 0.05984718680711576,\n 'abtest': 0.06071818811404983,\n 'fuelType': 0.060080938385732696,\n 'gearbox': 0.06072952180624125,\n 'notRepairedDamage': 0.06106010397906847,\n 'holidays': 0.05685565776346668,\n 'price_na': 0.060948957015006355,\n 'age_na': 0.06027832457888893,\n 'powerPS_na': 0.06106806907313633}\n\n\n\n{k:oob_scores[k] for k in oob_scores if oob_scores[k] > original_oob}\n\n{'abtest': 0.06071818811404983,\n 'gearbox': 0.06072952180624125,\n 'notRepairedDamage': 0.06106010397906847,\n 'price_na': 0.060948957015006355,\n 'age_na': 0.06027832457888893,\n 'powerPS_na': 0.06106806907313633}\n\n\nIt seems that dropping the notRepairedDamage, abtest, and powerPS_na might be beneficial\n\ndef evaluate_removing_redundant (to_drop, X_imp, y, valid_X, valid_y):\n    print ('OOB score: ', get_oob(X_imp.drop(to_drop, axis=1), y))\n    X_final = X_imp.drop(to_drop, axis=1)\n    valid_X_final = valid_X_imp.drop(to_drop, axis=1)\n    rf_model = fitted_rf(X_final, y)\n    print ('Regression errors: ', m_rmse(rf_model, X_final, y), m_rmse(rf_model, valid_X_final, valid_y), r_mse(rf_model.oob_prediction_, y))\n    \n    return rf_model, X_final, valid_X_final\n\n\nrf_model, X_final, valid_X_final = evaluate_removing_redundant (['abtest', 'notRepairedDamage', 'powerPS_na'], X_imp, y, valid_X, valid_y)\njoblib.dump ([rf_model, X_final, valid_X_final], results_path / 'removing_redundant.pkl')\n\nOOB score:  0.0611047486037124\nRegression errors:  6.050136 8.261787 8.202955\n\n\n['results/used_cars/removing_redundant.pkl']\n\n\n\nrf_model2, X_final2, valid_X_final2 = evaluate_removing_redundant (['notRepairedDamage', 'powerPS_na'], X_imp, y, valid_X, valid_y)\n\nOOB score:  0.0606092565973716\nRegression errors:  6.065446 8.26426 8.207189\n\n\n\nrf_model2, X_final2, valid_X_final2 = evaluate_removing_redundant (['notRepairedDamage'], X_imp, y, valid_X, valid_y)\n\nOOB score:  0.060027249156653584\nRegression errors:  6.03148 8.262977 8.207328\n\n\n\nrf_model2, X_final2, valid_X_final2 = evaluate_removing_redundant (['abtest'], X_imp, y, valid_X, valid_y)\n\nOOB score:  0.060528790706093516\nRegression errors:  6.037068 8.266976 8.205677\n\n\nWe select the model without [‘abtest’, ‘notRepairedDamage’, ‘powerPS_na’]. The result is slightly better, with lower number of variables, which tends to be good."
  },
  {
    "objectID": "analysis.html#neural-network",
    "href": "analysis.html#neural-network",
    "title": "Analysis",
    "section": "Neural Network",
    "text": "Neural Network\nWe use a similar preprocessing as the one used for RF, but with two important differences: - The numerical variables are standard normalized. - The categorical variables will be passed to an embedding layer in the Neural Network, which maps discrete integers to embeddings that have been learned to optimally represent each category in the variable. This tends to be a bit more effective and efficient than using one-hot-encoding.\n\nselected_columns = set (X_final.columns).intersection(df_clean.columns)\ndf_nn = (df_clean[list(selected_columns) + [target_variable]]).copy()\ndf_nn[target_variable] = df_nn[target_variable].astype (np.float32)\ncategorical_variables = set (selected_columns).intersection (categorical_variables)\nnumerical_variables = set (selected_columns).intersection (numerical_variables)\nprocs = [Categorify, FillMissing, Normalize]\ntabular = TabularPandas (df_nn, procs, list(categorical_variables), list(numerical_variables), y_names=target_variable, splits=(list(train_idx),list(valid_idx)))\n\nWe use a relatively high batch-size due to the fact that tabular data does not occupy much memory.\n\ndls = tabular.dataloaders(1024)\ny = tabular.train.y\nlearn = tabular_learner(dls, y_range=(np.floor(y.min()), np.ceil(y.max())), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.001737800776027143)\n\n\n\n\n\nWe try first with learning rate 1e-2 and 10 epochs:\n\nlearn.fit_one_cycle(10, 1e-2)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      602.610474\n      76.169884\n      00:11\n    \n    \n      1\n      78.452293\n      79.093147\n      00:10\n    \n    \n      2\n      72.395233\n      75.213257\n      00:10\n    \n    \n      3\n      68.382301\n      73.556313\n      00:10\n    \n    \n      4\n      64.683945\n      75.480438\n      00:10\n    \n    \n      5\n      61.083923\n      77.244652\n      00:10\n    \n    \n      6\n      50.728420\n      79.223137\n      00:11\n    \n    \n      7\n      43.104126\n      84.684929\n      00:10\n    \n    \n      8\n      34.933899\n      85.208206\n      00:10\n    \n    \n      9\n      30.915373\n      87.217155\n      00:10\n    \n  \n\n\n\nWe see that something close to 4 or 5 epochs is better\n\nlearn = tabular_learner(dls, y_range=(np.floor(y.min()), np.ceil(y.max())), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\nlearn.fit_one_cycle(4, 1e-2)\npreds, targs = learn.get_preds()\nr_mse (preds,targs)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      345.358978\n      78.925217\n      00:11\n    \n    \n      1\n      74.916069\n      75.385986\n      00:10\n    \n    \n      2\n      65.404991\n      74.644577\n      00:10\n    \n    \n      3\n      51.674995\n      76.773880\n      00:11\n    \n  \n\n\n\n\n\n\n\n\n\n\n8.762071\n\n\nThe validation error is 8.76, bigger than the one obtained by the RF. Let’s see with a lower learning rate:\n\nlearn2 = tabular_learner(dls, y_range=(np.floor(y.min()), np.ceil(y.max())), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\nlearn2.fit_one_cycle(10, 1e-3)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      12781.711914\n      7655.472656\n      00:11\n    \n    \n      1\n      333.679382\n      107.907936\n      00:11\n    \n    \n      2\n      68.636696\n      75.976501\n      00:11\n    \n    \n      3\n      60.806812\n      77.812447\n      00:10\n    \n    \n      4\n      54.039669\n      81.903381\n      00:10\n    \n    \n      5\n      46.701462\n      79.440315\n      00:10\n    \n    \n      6\n      37.569061\n      79.386604\n      00:10\n    \n    \n      7\n      29.916273\n      80.873795\n      00:10\n    \n    \n      8\n      24.533495\n      81.103035\n      00:11\n    \n    \n      9\n      21.414110\n      81.744980\n      00:10\n    \n  \n\n\n\nWe see that the loss is not better than using a bigger learning rate. Let’s see even bigger:\n\nlearn2 = tabular_learner(dls, y_range=(np.floor(y.min()), np.ceil(y.max())), layers=[500,250],\n                        n_out=1, loss_func=F.mse_loss)\nlearn2.fit_one_cycle(4, 1e-1)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      198.138016\n      131.202820\n      00:10\n    \n    \n      1\n      75.276131\n      75.907112\n      00:11\n    \n    \n      2\n      113.341293\n      151.957809\n      00:11\n    \n    \n      3\n      150.795166\n      151.957809\n      00:10\n    \n  \n\n\n\nClearly, the best learning rate seems to be 0.01\nWe could now examine other hyper-parameters, like regularization, dropout, adding more layers or units per layer, adding batch-normalization, etc."
  },
  {
    "objectID": "analysis.html#ensemble-of-rf-and-nn",
    "href": "analysis.html#ensemble-of-rf-and-nn",
    "title": "Analysis",
    "section": "Ensemble of RF and NN",
    "text": "Ensemble of RF and NN\nWe see if using an ensemble of RF and NN improves the results:\n\npreds, targs = learn.get_preds()\n\nrf_preds = rf_model.predict(valid_X_final)\nens_preds = (to_np(preds.squeeze()) + rf_preds) /2\nr_mse(ens_preds,valid_y)\n\n\n\n\n\n\n\n\n8.339824\n\n\nWhile it improves the result of using NN, using RF alone seems to be a better option in this data."
  },
  {
    "objectID": "analysis.html#future-lines-of-work",
    "href": "analysis.html#future-lines-of-work",
    "title": "Analysis",
    "section": "Future lines of work",
    "text": "Future lines of work\n\nAs indicated above, we should now examine other hyper-parameters, like regularization, dropout, adding more layers or units per layer, adding batch-normalization, etc.\nGiven that a random forest seems to be the best type of model for this data, it is a good idea to explore using XGBoost, CatBoost, or other types of tree ensembles."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tabular ML",
    "section": "",
    "text": "This repository shows a data science pipeline applied to typical datasets such as the “used cars” dataset"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Tabular ML",
    "section": "Install",
    "text": "Install\ngit clone https://github.com/JaumeAmoresDS/tabularml.git\npip install -e tabularml[dev]"
  },
  {
    "objectID": "index.html#analysis",
    "href": "index.html#analysis",
    "title": "Tabular ML",
    "section": "Analysis",
    "text": "Analysis\nIn order to see the analysis currently performed on the “used cars” dataset, first download the data from https://data.world/data-society/used-cars-data, and then run the notebook in nbs/analysis.ipynb. The result of this analysis can be seen in html here: https://jaumeamoresds.github.io/tabularml/analysis.html"
  }
]